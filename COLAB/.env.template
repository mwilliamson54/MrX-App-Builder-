# MrX Colab Agent Configuration Template
# Copy this file to .env and fill in your values
# OR set these as environment variables in Colab

# =============================================================================
# REQUIRED SETTINGS
# =============================================================================

# Your Cloudflare Pages backend URL
BACKEND_URL=https://your-app.pages.dev

# Unique identifier for this Colab instance
COLAB_ID=colab-001

# Secret for claiming jobs from backend (get this from your Cloudflare setup)
CLAIM_SECRET=your-secret-here

# GitHub Personal Access Token (needs 'repo' permission)
# Get from: https://github.com/settings/tokens
GITHUB_PAT=ghp_your_github_token_here

# =============================================================================
# OPTIONAL: LLM CONFIGURATION (Choose One)
# =============================================================================

# Option 1: OpenAI API
# Get from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-your-openai-key-here

# Option 2: Custom LLM Endpoint (e.g., Claude via Anthropic)
# CUSTOM_LLM_ENDPOINT=https://api.anthropic.com/v1/messages
# CUSTOM_LLM_KEY=sk-ant-your-key-here

# Option 3: Other OpenAI-compatible endpoints
# CUSTOM_LLM_ENDPOINT=https://your-llm-provider.com/v1/chat/completions
# CUSTOM_LLM_KEY=your-api-key

# =============================================================================
# OPTIONAL: GOOGLE DRIVE CONFIGURATION
# =============================================================================

# Google Drive Service Account Credentials (JSON format)
# For APK uploads - get from Google Cloud Console
# GOOGLE_DRIVE_CREDENTIALS={"type":"service_account","project_id":"..."}

# =============================================================================
# OPTIONAL: PERFORMANCE TUNING
# =============================================================================

# Embedding model to use (default: mini)
# Options: mini, bge-small, bge-base, mpnet, instructor
# DEFAULT_EMBEDDING_MODEL=mini

# How often to poll for jobs (seconds)
# POLL_INTERVAL=30

# Maximum time for Gradle builds (seconds)
# GRADLE_TIMEOUT=600

# Maximum time for LLM calls (seconds)
# LLM_TIMEOUT=60

# Build variant (release or debug)
# BUILD_VARIANT=release

# Number of code chunks to retrieve for context
# TOP_K_CHUNKS=15

# Embedding batch size
# EMBEDDING_BATCH_SIZE=32

# Maximum context tokens for LLM
# MAX_CONTEXT_TOKENS=8000

# Maximum LLM response tokens
# LLM_MAX_TOKENS=1800

# LLM temperature (0.0 - 1.0)
# LLM_TEMPERATURE=0.2

# =============================================================================
# OPTIONAL: FAISS INDEX CONFIGURATION
# =============================================================================

# Index type (IndexFlatL2 or HNSW)
# FAISS_INDEX_TYPE=IndexFlatL2

# Number of FAISS index versions to keep
# KEEP_LAST_N_INDEXES=3

# =============================================================================
# OPTIONAL: GIT CONFIGURATION
# =============================================================================

# Git user name for commits
# GIT_USER_NAME=MrX Bot

# Git user email for commits
# GIT_USER_EMAIL=bot@mrxapp.dev

# =============================================================================
# OPTIONAL: LOGGING
# =============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# LOG_LEVEL=INFO

# Log lines per chunk
# LOG_CHUNK_SIZE=100

# Maximum log chunk size (bytes)
# LOG_MAX_SIZE=5120

# =============================================================================
# NOTES
# =============================================================================

# Security Tips:
# - Never commit this file with real credentials to GitHub
# - Use Colab Secrets for sensitive data (recommended)
# - Rotate tokens regularly
# - Use minimal GitHub PAT permissions

# Performance Tips:
# - Start with 'mini' embedding model (fastest)
# - Increase POLL_INTERVAL if you have few jobs
# - Use 'debug' build variant for faster testing
# - Reduce TOP_K_CHUNKS if LLM context is too large

# Troubleshooting:
# - If builds timeout, increase GRADLE_TIMEOUT
# - If out of memory, use smaller embedding model
# - If slow, reduce EMBEDDING_BATCH_SIZE
# - Check logs with LOG_LEVEL=DEBUG