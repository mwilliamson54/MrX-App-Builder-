MrX App Builder Platform — COLAB HEAVY AGENT SPECIFICATION (Deep Expanded Developer Blueprint)

PURPOSE
The Colab Heavy Agent is the computational backbone of the entire MrX App Builder Platform. It is responsible for all expensive or resource-intensive tasks including embedding generation, FAISS index management, Tree-Sitter parsing, retrieval-augmented prompt assembly, LLM-based code transformation, patch creation, Gradle builds, error repair loops, APK artifact generation, and secure integration with GitHub + Google Drive. The platform’s free-first design requires that 100% of heavy workloads run in Colab to avoid costs associated with hosted compute or vector databases.

HIGH-LEVEL RESPONSIBILITIES
• Pull project source code from GitHub and maintain a synchronized local working copy.  
• Parse project structure using Tree-Sitter (Java, Kotlin, XML, Groovy).  
• Chunk files into logical units (classes, functions, XML components) with metadata.  
• Generate local embeddings using free open-source models (MiniLM-L6, BGE-small/base, MPNet, Instructor-large).  
• Build, update, and query FAISS indexes for each project.  
• Assemble minimal retrieval-first LLM context for each chat request.  
• Call LLM using either user-provided OpenAI key or preferred custom LLM endpoint.  
• Generate patch diffs, validate patches, and commit changes to GitHub.  
• Execute Gradle builds, collect errors, attempt auto-fixing loops (up to 3).  
• Upload APKs to Google Drive under fixed structure.  
• Stream logs and job updates to Workers KV.  
• Claim & execute jobs from backend job queue.  
• Maintain state consistency, cleanup, and safety of each Colab session.

COLAB STARTUP PROCEDURE
1. Authenticate with backend using a one-time secure claim secret.  
2. Fetch encrypted GitHub PAT + Google Drive credentials from backend’s admin-secret endpoint.  
3. Set environment variables for GitHub, Drive, and LLM.  
4. Clone or shallow-fetch GitHub repo for the selected project.  
5. Initialize local directories: /workspace/src, /workspace/faiss, /workspace/logs, /workspace/build.  
6. Download or rebuild FAISS index if needed; validate index version with KV manifest.  
7. Load local embedding model (MiniLM/BGE/MPNet/Instructor) and warm it up.  
8. Initialize Tree-Sitter parsers for Java/Kotlin/XML/Groovy.

PROJECT CODE PARSING & CHUNK GENERATION
Tree-Sitter is used for semantic code segmentation:
• Java/Kotlin: classes, methods, constructors, imports, annotations.  
• XML: view nodes, attributes, layout blocks.  
• Groovy: Gradle tasks, dependency blocks, plugin declarations.  

Each chunk contains:
{
  "chunkId": "...",
  "path": "app/src/main/java/.../MainActivity.kt",
  "nodeType": "method_declaration",
  "startLine": 120,
  "endLine": 172,
  "tokens": "<raw snippet>",
  "timestamp": 1690002211
}

Chunk embeddings are generated using a local model and fed into FAISS. Metadata stored in KV.

EMBEDDINGS & FAISS INDEXING
Requirements:
• Only open-source models allowed.  
• Batch embeddings for efficiency (batch size 16–64).  
• FAISS index type: IndexFlatL2 or HNSW for large repos.  
• Index refresh triggered on new Git commits, patches, or build operations.  
• Manifest stored in KV:
{
  "indexVersion": "7",
  "numChunks": 432,
  "updatedAt": "2025-11-22T10:05Z"
}

RETRIEVAL-FIRST PROMPT CONSTRUCTION
For any LLM request:
1. Extract recent chat messages from KV (last N messages or vector-summarized history).  
2. Convert user request into embedding vector.  
3. Query FAISS for top-K chunks (K ~ 8–20).  
4. Build structured prompt containing:
   • system instructions  
   • user intention  
   • retrieved code fragments (path + snippet)  
   • project configuration extracted from mrx-config.json  
   • style constraints + safety checks  
5. DO NOT ever send entire files to LLM.  
6. Enforce token limits to avoid overuse and maximize relevance.

LLM CALL LOGIC
Two supported modes:
• customLLM: custom base URL + full Authorization header (preferred).  
• openai: requires user-supplied key (optional).

LLM request payload:
{
  "model": "...",
  "messages": [...],
  "max_tokens": 1800,
  "temperature": 0.2,
  "presence_penalty": 0.0,
  "frequency_penalty": 0.0
}

PATCH GENERATION + UNIFIED DIFF
Colab reads LLM response and attempts to produce valid diffs:
1. Extract <file>, <before>, <after> from LLM output.  
2. Build a diff using python-difflib or unified diff tools.  
3. Validate patch by applying it to working copy.  
4. If patch fails, re-run with error context.  
5. After 3 failures → job flagged as user-fix-required.

PATCH COMMIT & GITHUB INTEGRATION
• Branch naming: mrx/update/{project}/{taskType}/{timestamp}.  
• Commit pattern:
  - "feat(mrx): applied automated patch for issue XXXXX"  
• Push via GitHub PAT stored safely in Colab environment.  
• Optional: create PR or directly push to default branch (configuration-driven).

GRADLE BUILD AUTOMATION
Commands:
• ./gradlew assembleRelease  
• ./gradlew assembleDebug  
• ./gradlew lint  
• ./gradlew test  

Build pipeline:
1. Run assemble.  
2. Capture stdout/stderr logs.  
3. If errors detected, extract stacktrace → feed back into LLM for auto-repair.  
4. Retry up to 3 cycles.  
5. On success, gather APK path: app/build/outputs/apk/release/*.apk.

APK UPLOAD TO GOOGLE DRIVE
Upload path:
"MrX App Builder / {ProjectName} / {ProjectName}-{BuildName}.apk"

Drive interactions performed using OAuth tokens stored in Colab environment.  
File metadata (sha256, size, url, uploadedAt) posted to KV.

JOB CLAIMING PROTOCOL
1. Colab sends /jobs/claim with colabId + claimSecret.  
2. Backend returns next pending job or null.  
3. Colab marks job as running, fetches payload.  
4. All lifecycle updates posted to KV: progress, logs, completion, errors.

LOGGING & STREAMING
All operations produce log lines:
{
  "timestamp": "...",
  "level": "INFO|WARN|ERROR",
  "component": "colab-agent",
  "msg": "Building APK...",
  "meta": {...}
}

Logs chunked and appended to KV:
project:{projectId}:logs:{jobId}:segment:{n}

ERROR HANDLING & CLEANUP
• Automatic retry for patch/build jobs (max 3).  
• Remove orphan branches older than TTL.  
• Rebuild FAISS index if corrupted.  
• Reset Colab environment if memory exceeded.

DEVELOPER CHECKLIST (CRITICAL)
• Must implement Tree-Sitter parsing & chunking.  
• Must generate embeddings locally; never call external embedding APIs.  
• Must use FAISS for vector search.  
• Must ensure patch validation before committing.  
• Must store logs, not raw files, in KV.  
• Must upload artifacts only to Drive, never to backend.  
• Must avoid exposing secrets to frontend at all costs.  
• Must support multi-chat workflows (separate conversation context per chatId).

CONCLUSION
The Colab Heavy Agent is the intelligence and muscle behind the entire MrX App Builder ecosystem. It performs advanced RAG-driven code analysis, ensures reliable automated builds, executes patch workflows, and integrates cleanly with GitHub and Google Drive. By enforcing local embeddings, FAISS search, Tree-Sitter parsing, and a retrieval-first LLM pipeline, the agent guarantees efficient, accurate, and cost-free operation at scale.