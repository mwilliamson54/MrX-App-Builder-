# MrX App Builder - Colab Heavy Agent
# Complete implementation notebook

# Cell 1: Install Dependencies
"""
Run this cell first to install all required packages
"""
!pip install -q tree-sitter tree-sitter-languages
!pip install -q sentence-transformers transformers torch
!pip install -q faiss-cpu
!pip install -q GitPython
!pip install -q google-api-python-client google-auth google-auth-oauthlib
!pip install -q openai requests tenacity diff-match-patch tqdm pydantic python-dotenv

print("✓ Dependencies installed")

# ============================================================================
# Cell 2: Setup and Configuration
"""
Configure your Colab agent here.
Set your credentials in the Colab Secrets or directly here.
"""

import os
from pathlib import Path

# IMPORTANT: Set these values
BACKEND_URL = "https://your-app.pages.dev"  # Your Cloudflare Pages URL
COLAB_ID = "colab-001"  # Unique ID for this Colab instance
CLAIM_SECRET = "your-claim-secret"  # Secret for job claiming

# Set environment variables
os.environ["BACKEND_URL"] = BACKEND_URL
os.environ["COLAB_ID"] = COLAB_ID
os.environ["CLAIM_SECRET"] = CLAIM_SECRET

# Optional: Set secrets directly (or use Colab Secrets)
# GitHub Personal Access Token (required)
# os.environ["GITHUB_PAT"] = "ghp_your_token_here"

# Google Drive credentials (optional, for APK upload)
# os.environ["GOOGLE_DRIVE_CREDENTIALS"] = '{"type": "service_account", ...}'

# LLM Configuration (optional)
# Option 1: Custom LLM endpoint
# os.environ["CUSTOM_LLM_ENDPOINT"] = "https://your-llm-api.com/v1/chat/completions"
# os.environ["CUSTOM_LLM_KEY"] = "your-api-key"

# Option 2: OpenAI
# os.environ["OPENAI_API_KEY"] = "sk-your-openai-key"

print("✓ Configuration set")

# ============================================================================
# Cell 3: Import All Modules
"""
Import the MrX agent modules
"""

import sys
sys.path.append('/content')

# Import all modules (assuming code is in /content/)
from config.settings import settings
from config.secrets import secret_manager
from utils.logger import logger
from core.auth import authenticator
from core.job_manager import job_manager, Job
from storage.kv_client import kv_client
from storage.log_streamer import LogStreamingContext
from parsers.tree_sitter_parser import ts_parser
from chunking.chunker import CodeChunker
from embeddings.model_loader import get_embedding_model, BatchEmbedder
from vector.faiss_manager import get_faiss_index
from git.repo_manager import get_repo_manager
from llm.prompt_builder import prompt_builder
from llm.llm_client import llm_client, response_parser
from build.gradle_executor import build_project, ErrorParser

print("✓ Modules imported")

# ============================================================================
# Cell 4: Initialize Agent
"""
Initialize the Colab agent and authenticate
"""

def initialize_agent():
    """Initialize agent and authenticate"""
    logger.info("Initializing MrX Colab Agent")
    
    # Create workspace directories
    settings.create_directories()
    logger.info("Workspace directories created")
    
    # Authenticate with backend
    try:
        success = authenticator.setup_session()
        if not success:
            logger.error("Authentication failed")
            return False
            
        logger.info("✓ Authentication successful")
        
    except Exception as e:
        logger.error(f"Initialization failed: {e}")
        return False
        
    # Load embedding model
    try:
        logger.info("Loading embedding model...")
        embedding_model = get_embedding_model()
        logger.info(f"✓ Model loaded: {embedding_model.get_model_name()}")
        
    except Exception as e:
        logger.error(f"Failed to load embedding model: {e}")
        return False
        
    logger.info("✓ Agent initialized successfully")
    return True

# Run initialization
if initialize_agent():
    print("\n✅ Agent is ready!")
else:
    print("\n❌ Initialization failed - check logs above")

# ============================================================================
# Cell 5: Job Execution Logic
"""
Core job execution pipeline
"""

def execute_job(job: Job):
    """
    Execute a job from the queue
    
    This is the main entry point for all job types:
    - build-and-patch: Generate code patches and build APK
    - build-only: Just build the project
    - index-update: Update FAISS index only
    """
    logger.info(f"Executing job: {job.id} (type: {job.type})")
    
    # Mark job as running
    job_manager.mark_running()
    
    # Get project metadata
    project_meta = kv_client.get_project_meta(job.project_id)
    if not project_meta:
        job_manager.mark_failed("Project metadata not found")
        return
        
    # Start log streaming
    with LogStreamingContext(job.project_id, job.id) as log_stream:
        
        try:
            # Step 1: Clone/update repository
            logger.info("Step 1: Repository setup")
            repo_manager = get_repo_manager(
                job.project_id,
                project_meta.get("repoUrl")
            )
            
            if not repo_manager.is_initialized():
                if not repo_manager.clone():
                    raise Exception("Failed to clone repository")
            else:
                repo_manager.fetch()
                
            log_stream.add_log(logger.get_buffer())
            logger.clear_buffer()
            
            # Step 2: Parse and chunk code
            logger.info("Step 2: Parsing project")
            chunker = CodeChunker(
                job.project_id,
                repo_manager.get_repo_path()
            )
            
            filters = job.payload.get("retrievalFilters")
            chunks = chunker.chunk_project(filters)
            
            logger.info(f"Created {len(chunks)} chunks")
            log_stream.add_log(logger.get_buffer())
            logger.clear_buffer()
            
            # Step 3: Update FAISS index
            logger.info("Step 3: Updating vector index")
            embedding_model = get_embedding_model()
            embedder = BatchEmbedder(embedding_model)
            
            # Generate embeddings
            embeddings = embedder.embed_chunks(chunks)
            
            # Get or create FAISS index
            faiss_index = get_faiss_index(
                job.project_id,
                embedding_model.get_dimension()
            )
            
            # Clear and rebuild index
            faiss_index.clear()
            faiss_index.create_index()
            
            # Add vectors
            chunk_metadata = [chunk.to_dict() for chunk in chunks]
            faiss_index.add_vectors(embeddings, chunk_metadata)
            
            # Save index
            faiss_index.save()
            
            logger.info("Index updated successfully")
            log_stream.add_log(logger.get_buffer())
            logger.clear_buffer()
            
            # Step 4: Handle job type
            if job.type == "build-and-patch":
                result = execute_build_and_patch(
                    job, repo_manager, faiss_index, embedder, log_stream
                )
            elif job.type == "build-only":
                result = execute_build_only(
                    job, repo_manager, log_stream
                )
            elif job.type == "index-update":
                result = {"status": "completed", "chunks": len(chunks)}
            else:
                raise Exception(f"Unknown job type: {job.type}")
                
            # Mark as completed
            job_manager.mark_completed(result)
            logger.info("Job completed successfully")
            
        except Exception as e:
            logger.error(f"Job failed: {str(e)}")
            log_stream.add_log(logger.get_buffer())
            job_manager.mark_failed(str(e))

def execute_build_and_patch(job, repo_manager, faiss_index, embedder, log_stream):
    """Execute build-and-patch job"""
    
    # Get user request
    user_message = job.payload.get("patchRequest", "")
    
    # Retrieve relevant chunks
    logger.info("Retrieving relevant code")
    query_embedding = embedder.embed_query(user_message)
    retrieved_chunks = faiss_index.search(query_embedding)
    
    # Build prompt
    messages = prompt_builder.build_prompt(
        user_message,
        retrieved_chunks,
        chat_history=job.payload.get("chatHistory"),
        project_config=job.payload.get("projectConfig")
    )
    
    # Call LLM
    logger.info("Calling LLM")
    response = llm_client.call_llm(messages)
    
    if not response:
        raise Exception("LLM call failed")
        
    # Extract patches
    patches = response_parser.extract_patches(response)
    logger.info(f"Extracted {len(patches)} patches")
    
    log_stream.add_log(logger.get_buffer())
    logger.clear_buffer()
    
    # Apply patches (simplified - full implementation needed)
    # TODO: Implement patch application
    
    # Build project
    logger.info("Building project")
    success, output, apks = build_project(
        repo_manager.get_repo_path(),
        variant=job.payload.get("buildVariant", "release")
    )
    
    if not success:
        # Try auto-fix (simplified)
        errors = ErrorParser.parse_errors(output, "")
        logger.warning(f"Build failed with {len(errors)} errors")
        # TODO: Implement auto-fix loop
        
    log_stream.add_log(logger.get_buffer())
    
    return {
        "status": "completed",
        "patches": len(patches),
        "build_success": success,
        "apks": [str(apk) for apk in apks]
    }

def execute_build_only(job, repo_manager, log_stream):
    """Execute build-only job"""
    
    logger.info("Building project (no patches)")
    
    success, output, apks = build_project(
        repo_manager.get_repo_path(),
        variant=job.payload.get("buildVariant", "release"),
        clean=job.payload.get("clean", False)
    )
    
    log_stream.add_log(logger.get_buffer())
    
    return {
        "status": "completed",
        "build_success": success,
        "apks": [str(apk) for apk in apks]
    }

print("✓ Job execution logic loaded")

# ============================================================================
# Cell 6: Start Agent (Main Loop)
"""
Start the main polling loop
This cell will run indefinitely, polling for jobs
"""

def main():
    """Main agent loop"""
    logger.info("=" * 60)
    logger.info("MrX Colab Agent Started")
    logger.info(f"Backend: {settings.BACKEND_URL}")
    logger.info(f"Colab ID: {settings.COLAB_ID}")
    logger.info("=" * 60)
    
    try:
        # Start polling loop
        job_manager.poll_loop(
            callback=execute_job,
            interval=settings.POLL_INTERVAL
        )
    except KeyboardInterrupt:
        logger.info("Agent stopped by user")
    except Exception as e:
        logger.error(f"Agent crashed: {str(e)}")
        raise

# Run the agent
print("Starting agent... (Press Stop button to halt)")
print("-" * 60)
main()